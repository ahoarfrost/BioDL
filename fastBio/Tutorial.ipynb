{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastBio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BioTokenizer' from 'fastBio' (/Users/adrienne/anaconda3/envs/testpypi3/lib/python3.7/site-packages/fastBio/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-aa9ec6d443d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfastBio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBioTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBioVocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BioTokenizer' from 'fastBio' (/Users/adrienne/anaconda3/envs/testpypi3/lib/python3.7/site-packages/fastBio/__init__.py)"
     ]
    }
   ],
   "source": [
    "from fastBio import BioTokenizer, BioVocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps to training a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fast.ai, there are three basic steps to training a deep learning model: \n",
    "\n",
    "1) Define your **transforms** (for sequences/text, this means defining the **tokenizer** and **vocabulary** you will use for tokenization and numericalization)\n",
    "\n",
    "2) Create a **Databunch** (which wraps up a Pytorch Dataset and Dataloader into one)\n",
    "\n",
    "3) Create a **Learner** with your specified **model config**\n",
    "\n",
    "and train!\n",
    "\n",
    "If fastai v1 is new to you, I recommend taking a look at their very extensive [documentation](https://fastai1.fast.ai/), [forum](https://forums.fast.ai/), and [online course](https://course19.fast.ai/). Note fastBio uses fastai v1, which isn't compatible with the new fastai v2.\n",
    "\n",
    "Biological sequence data asks for some special treatment as compared to text (kmer-based tokenization; handling sequence file types like fasta/fastq), so while we can use much of the built-in fast.ai *text* functionality, fastBio provides some helper functions and classes to deal with some of the quirks of biological data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create tokenizer and vocabulary for transforming seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioTokenizer with the following special tokens:\n",
       " - xxunk\n",
       " - xxpad\n",
       " - xxbos\n",
       " - xxeos"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define a tokenizer with the correct kmer size and stride for your data\n",
    "\n",
    "from fastBio.transform import BioTokenizer, BioVocab\n",
    "tok = BioTokenizer(ksize=1, stride=1)\n",
    "tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kmer size is how many nucleotides constitute a 'word' in the sequence, and the stride is the number of nucleotides to skip between tokens. \n",
    "\n",
    "So for a sequence: `ACGGCGCTC`\n",
    "\n",
    "a kmer size of 3 and stride of 1 would result in the tokenized sequence: `['ACG','CGG','GGC','GCG','CGC','GCT','CTC']`\n",
    "\n",
    "whereas a kmer size of 3 and stride of 3 would result in: `['ACG','GCG','CTC']`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create vocab from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'A', 'C', 'T', 'G']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'xxunk': 0,\n",
       "             'xxpad': 1,\n",
       "             'xxbos': 2,\n",
       "             'xxeos': 3,\n",
       "             'A': 4,\n",
       "             'C': 5,\n",
       "             'T': 6,\n",
       "             'G': 7})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_voc = BioVocab.create_from_ksize(ksize=1)\n",
    "print(model_voc.itos)\n",
    "model_voc.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above I created a vocabulary using a kmer size of 1 (so just the nucleotides A, C, T, G), but you can use larger kmer sizes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk', 'xxpad', 'xxbos', 'xxeos', 'AC', 'CT', 'TA', 'TT', 'CC', 'AT', 'GG', 'AA', 'GC', 'GT', 'AG', 'TC', 'CG', 'TG', 'CA', 'GA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'xxunk': 0,\n",
       "             'xxpad': 1,\n",
       "             'xxbos': 2,\n",
       "             'xxeos': 3,\n",
       "             'AC': 4,\n",
       "             'CT': 5,\n",
       "             'TA': 6,\n",
       "             'TT': 7,\n",
       "             'CC': 8,\n",
       "             'AT': 9,\n",
       "             'GG': 10,\n",
       "             'AA': 11,\n",
       "             'GC': 12,\n",
       "             'GT': 13,\n",
       "             'AG': 14,\n",
       "             'TC': 15,\n",
       "             'CG': 16,\n",
       "             'TG': 17,\n",
       "             'CA': 18,\n",
       "             'GA': 19})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_voc = BioVocab.create_from_ksize(ksize=2)\n",
    "print(model_voc.itos)\n",
    "model_voc.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or download the predefined LookingGlass vocabulary\n",
    "\n",
    "For training the LookingGlass model, I used a ksize=1, stride=1. If you're using a pretrained LookingGlass-based model, you want to make sure that your vocabulary is in the same order so that numericalization is the same for your data as for the LookingGlass weights. \n",
    "\n",
    "Or, it's easy to simply download the LookingGlass vocabulary for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['xxunk' 'xxpad' 'xxbos' 'xxeos' 'G' 'A' 'C' 'T']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'xxunk': 0,\n",
       "             'xxpad': 1,\n",
       "             'xxbos': 2,\n",
       "             'xxeos': 3,\n",
       "             'G': 4,\n",
       "             'A': 5,\n",
       "             'C': 6,\n",
       "             'T': 7})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or download from pretrained vocab used in LookingGlass\n",
    "\n",
    "#you might need this if you are me...\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve (\"https://github.com/ahoarfrost/LookingGlass/releases/download/v1.0/ngs_vocab_k1_withspecial.npy\", \"ngs_vocab_k1_withspecial.npy\")\n",
    "\n",
    "import numpy as np\n",
    "voc = np.load('ngs_vocab_k1_withspecial.npy')\n",
    "model_voc = BioVocab(voc)\n",
    "print(model_voc.itos)\n",
    "model_voc.stoi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the order of the nucleotides in the vocabulary is different than the one that we generated from scratch; if you're using the pretrained LookingGlass-based models, make sure you're using the LookingGlass vocab described here as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a databunch \n",
    "\n",
    "You can create a databunch using the **BioLMDataBunch** (for language modeling) or **BioClasDataBunch** (for classification). You can do this from raw sequence data fasta/fastq files or csv files:\n",
    "\n",
    "* from_folder\n",
    "* from_seqfile\n",
    "* from_df\n",
    "* from_multiple_csv\n",
    "\n",
    "You will probably want to create a **BioLMDataBunch** from_folder (which will include all sequences from a folder containing multiple fasta/fastq files), or from_seqfile (all sequences from a single fasta or fastq file). \n",
    "\n",
    "For a **BioClasDataBunch**, I find it easiest in practice to convert sequence files like fasta/fastq to csv files with the label in a column and the sequence in another column, and use from_df or from_multiple_csv, rather than use from_seqfile or from_folder. Alternatively, you *can* use the **BioTextList** class to go straight from sequence files. \n",
    "\n",
    "You can create a custom databunch, a la the fast.ai data block API, using the **BioTextList** class, which provides a few extra specialized labeling functions etc. If you *must* use sequence files for classification, for example, you can provide a fairly complicated regex-based function to use fastai's label_from_func, or create a BioTextList.from_folder and use label_from_fname or label_from_header in the BioTextList class to extract labels from a filename or fasta header, for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioLMDataBunch example\n",
    "\n",
    "Here we'll download some toy metagenomes (a small subset of sequences from 6 marine metagenomes from the [TARA project](https://www.ebi.ac.uk/ena/browser/view/PRJEB402)), split them into 'train' and 'valid' folders, and create a BioLMDataBunch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading ERR598981 ...\n",
      "downloading ERR599020 ...\n",
      "downloading ERR599039 ...\n",
      "downloading ERR599052 ...\n",
      "downloading ERR599063 ...\n",
      "downloading ERR599115 ...\n",
      "creating databunch\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BioLMDataBunch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-677c961f5517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#create new training chunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'creating databunch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m lmdata = BioLMDataBunch.from_folder(path=data_path, \n\u001b[0m\u001b[1;32m     37\u001b[0m                                         \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mksize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                         \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_voc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BioLMDataBunch' is not defined"
     ]
    }
   ],
   "source": [
    "#these are 1000 random sequences from 6 marine metagenomes from the TARA project:\n",
    "from pathlib import Path\n",
    "Path('./lmdata/train').mkdir(parents=True, exist_ok=True)\n",
    "Path('./lmdata/valid').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for srr in ['ERR598981','ERR599020','ERR599039','ERR599052']:\n",
    "    print('downloading',srr,'...')\n",
    "    'https://raw.githubusercontent.com/ahoarfrost/fastBio/master/example_data/TARA_cut1000/'+srr+'_cut1000.fastq'\n",
    "    url = 'https://raw.githubusercontent.com/ahoarfrost/fastBio/master/example_data/TARA_cut1000/'+srr+'_cut1000.fastq'\n",
    "    urllib.request.urlretrieve (url, Path('./lmdata/train/'+srr+'_cut1000.fastq'))\n",
    "for srr in ['ERR599063','ERR599115']:\n",
    "    print('downloading',srr,'...')\n",
    "    url = 'https://raw.githubusercontent.com/ahoarfrost/fastBio/master/example_data/TARA_cut1000/'+srr+'_cut1000.fastq'\n",
    "    urllib.request.urlretrieve (url, Path('./lmdata/valid/'+srr+'_cut1000.fastq'))\n",
    "\n",
    "data_path = Path('./lmdata/')\n",
    "train_path = Path('./train/') #relative to data_path\n",
    "valid_path = Path('./valid/')\n",
    "data_outfile = Path('metagenome_LMbunch.pkl')\n",
    "\n",
    "#define your batch size, ksize, and bptt\n",
    "bs=512 \n",
    "bptt=100\n",
    "ksize=1\n",
    "\n",
    "max_seqs=None #None or int to optionally limit the number of sequences read from each file in training\n",
    "val_max_seqs=None #same for valid set\n",
    "skiprows = 0 #0 or int to optionally skip X sequences in the beginning of the file before reading into the databunch\n",
    "val_skiprows = 0 #same for valid set\n",
    "#these will default to the parameters chosen here, we don't technically need to pass them\n",
    "\n",
    "#using tok and model_voc defined above\n",
    "\n",
    "#create new training chunk \n",
    "print('creating databunch')\n",
    "lmdata = BioLMDataBunch.from_folder(path=data_path, \n",
    "                                        train=train_path, valid=valid_path, ksize=ksize,\n",
    "                                        tokenizer=tok, vocab=model_voc,\n",
    "                                        max_seqs_per_file=max_seqs, val_maxseqs=val_max_seqs,\n",
    "                                        skiprows=skiprows, val_skiprows=val_skiprows,\n",
    "                                        bs=bs, bptt=bptt\n",
    "                                            )\n",
    "print('there are',len(lmdata.items),'items in itemlist, and',len(lmdata.valid_ds.items),'items in lmdata.valid_ds')\n",
    "print('databunch preview:')\n",
    "print(lmdata)\n",
    "#you can save your databunch to file like so:\n",
    "lmdata.save(data_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BioClasDataBunch example\n",
    "\n",
    "Here we'll download sequences that I've preprocessed: downloading the coding sequences of three genomes, splitting them into read-length chunks, and recording that sequence, along with the label of the known reading frame from the coding sequence, in a csv file for each sequence. The sequence is in a column named 'seq' and the label is in a column named 'frame'.\n",
    "\n",
    "We'll split these csv files into train and valid folders and create a BioClasDataBunch from_multiple_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 16204 items in itemlist, and 7347 items in data.valid_ds\n",
      "there are 6 classes\n",
      "databunch preview:\n",
      "TextClasDataBunch;\n",
      "\n",
      "Train: LabelList (16204 items)\n",
      "x: BioTextList\n",
      "xxbos A G T T A A A T C G A T T T G G G T T C C A A T A A A A A A T T T T A T A G C A A G T G T A T C A G T T A A A A T T G A A T A C T T G G T A A T G T A A A T A G T G A A A G C T A A A T T G A A A T A,xxbos A A A G A A G T C G A T A A T T T A T A G T A A A T A C T A T A G T T A T T A G G T A T G A A A T C A A T T T C A A A T T T G A A G G T A A T T A T G G G A A G A A T T G G A T A G A G A A A T G C A T G A G T T G T T T T T C C T G T A A A T A T A G T A G T T C T C A G,xxbos A A T G A A G T A T A G T G C T A T T T T A T T A A T A T G T A G C G T T A A T T T A T T T T G T T T T C A A A A T A A A T T A A C T A C T T C T C G A T G G G A A T T C C C T A A A G A A G A T T T A A T T A A A A A A A A A A T A A A A A T A G G C A T A A T T T A C C A T A A T T A C A T A A A T T C T A T C T T T T A C A A T G A A A A T T A T A A A T A C A T T G C C T T T A T C G G A A T A T T G A C A T C T T A T A A T G A A T G G A T T G A A A T A C A A T T T A G C C C C A T A A A T T T T T T T A C T A T C C C A A C A A A T A A A G A T T T T A T T T C A A A T A C T,xxbos C T A A T A T T G A A A A T G C T A T T A A A A A G T C T T T G A G T T C G G G T G T C A A T A T A G T A C T C A T T C C T T A G,xxbos T T G C A T C T T A T T T A T A A A A T T G G T G A A G T T C T T G C T A A A C A A T T G C G T A G A T T G G G T A T T A A T T T A A A T A T G G C T C C A G T T G C C G A T A T A A A A T T T G C A C C A C A T A C T C C T T T A T T A A A T A G G A C A T T T G G A G G A T A T T C C G C T T A T A A T\n",
      "y: CategoryList\n",
      "-1,-1,2,3,1\n",
      "Path: /Users/adrienne/Projects/fastBio/clasdata;\n",
      "\n",
      "Valid: LabelList (7347 items)\n",
      "x: BioTextList\n",
      "xxbos A T C T C T A C C A C C A A A T T C T T C T C C A A T T T G A G C T A A A G T G T G A T T T A A G A T C T C T T T T G T T A A A A A C A T T G C T A T A T G T C T T G C T G T T A C A A T T G A C T T A,xxbos A A G C T T T T A T A G C A G T T C A A A C C G T A A G T A A A A A T C C T G G A A T T T C T T A T A A T C C A T T G T T T A T T T A T G G T G A A T C T G G A A T G G G A A A A A C T C A T T T A T T A A A A G C T G C A A A A A A C T A T A T T G A A T C T A A T T T T T C T G A T C T A A A A G T T A,xxbos G T T C A T T A C T T G C A C C G A T T A C A A A A T T T T C A A A T G T G T T T T C A T T A A T T T T T T T A A C T T T T T T A G T G A T G A T A T C A G A A T G A T C T T T T T T G A T T A A T T C A T C T T T T T C T A G T T G T T T T T T A T A T T C T T G T T C G T A T G T A A A A C T A A T A T T,xxbos T T T A A A A T A C C T A A T T T T G A A G T A G G T A T A T C T C T A A A C A G A T C A G A A A C T A T T T C T A T A G T A A T A A T T T T T T C T T C T G G A T T T T G T T G A G A T C A A A A G T T T A A T C T T G A A A C A C T T C C T T T A A T T T T T C T A A C A T C A T C T G A A T A A T A A,xxbos T G T T A A A A A A A T T A A A G A A G T T G T T A G T G A A A A A T A T G G T A T T T C A G T T A A T G C A A T T G A T G G A A A A G C T A G A A G\n",
      "y: CategoryList\n",
      "-2,3,-1,-2,2\n",
      "Path: /Users/adrienne/Projects/fastBio/clasdata;\n",
      "\n",
      "Test: None\n"
     ]
    }
   ],
   "source": [
    "#these are 1000 random sequences from 6 marine metagenomes from the TARA project:\n",
    "\n",
    "Path('./clasdata/train').mkdir(parents=True, exist_ok=True)\n",
    "Path('./clasdata/valid').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for genome in ['GCA_000007025.1_ASM702v1','GCA_000008685.2_ASM868v2']:\n",
    "    print('downloading',genome,'...')\n",
    "    url = 'https://github.com/ahoarfrost/fastBio/raw/master/example_data/FrameClas_sample/'+genome+'.csv'\n",
    "    urllib.request.urlretrieve (url, Path('./clasdata/train/'+genome+'.csv'))\n",
    "\n",
    "url = 'https://github.com/ahoarfrost/fastBio/raw/master/example_data/FrameClas_sample/GCA_000011445.1_ASM1144v1.csv'\n",
    "print('downloading GCA_000011445.1_ASM1144v1...')    \n",
    "urllib.request.urlretrieve (url, Path('./clasdata/valid/GCA_000011445.1_ASM1144v1.csv'))\n",
    "\n",
    "data_path = Path('./clasdata/')\n",
    "train_path = Path('./clasdata/train/')\n",
    "valid_path = Path('./clasdata/valid/')\n",
    "data_outfile = Path('frameclas_bunch.pkl')\n",
    "\n",
    "#use tok and model_voc defined above again\n",
    "#you can optionally limit the number of sequences you read (and how many rows to skip in the csv)\n",
    "\n",
    "framedata = BioClasDataBunch.from_multiple_csv(path=data_path, train=train_path, valid=valid_path,\n",
    "                                    text_cols='seq', label_cols='frame',\n",
    "                                    tokenizer=tok, vocab=model_voc,\n",
    "                                    max_seqs_per_file=None, valid_max_seqs=None, skiprows=0, bs=512\n",
    "                                        )\n",
    "print('there are',len(framedata.items),'items in itemlist, and',len(framedata.valid_ds.items),'items in data.valid_ds')\n",
    "print('there are',framedata.c,'classes')\n",
    "\n",
    "print('databunch preview:')\n",
    "print(framedata)\n",
    "#you can save your databunch to file like so:\n",
    "framedata.save(data_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a learner and train\n",
    "\n",
    "You can now create a fastai 'learner' with your databunch and train! \n",
    "\n",
    "There's nothing special in fastBio you *need* to create a learner - you can use get_language_model or get_text_classifier and any model config you want (see the fastai and pytorch docs for this). \n",
    "\n",
    "To use LookingGlass architecture (with or without pretrained weights), use the **LookingGlass** or **LookingGlassClassifier** classes which maintain the architecture used for the LookingGlass models and associated transfer learning tasks.\n",
    "\n",
    "Make sure to use pretrained=False if you're not using a pretrained model (pretrained=True by default). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model\n",
    "\n",
    "Let's use our BioLMDataBunch to train a language model with the same architecture as LookingGlass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from scratch (no pretrained weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmlearn = LookingGlass(data=lmdata).load(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjusting batch size down for my laptop\n",
    "lmlearn.data.batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.592360</td>\n",
       "      <td>1.451439</td>\n",
       "      <td>0.302540</td>\n",
       "      <td>06:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.460893</td>\n",
       "      <td>1.411753</td>\n",
       "      <td>0.310217</td>\n",
       "      <td>06:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.421647</td>\n",
       "      <td>1.402959</td>\n",
       "      <td>0.316920</td>\n",
       "      <td>07:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.407578</td>\n",
       "      <td>1.399795</td>\n",
       "      <td>0.329194</td>\n",
       "      <td>06:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.403432</td>\n",
       "      <td>1.399376</td>\n",
       "      <td>0.330121</td>\n",
       "      <td>06:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lmlearn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using a pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `LookingGlass()` not found.\n"
     ]
    }
   ],
   "source": [
    "#create LookingGlass() model from databunch defined above\n",
    "lmlearn2 = LookingGlass().load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create LookingGlassClassifier() model from classifier databunch defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained model options:\n",
    "#    'LookingGlass'\n",
    "#    'LookingGlass_enc'\n",
    "#    'FunctionalClassifier_enc'\n",
    "#    'FunctionalClassifier'\n",
    "#    'OptimalTempClassifier'\n",
    "#    'OxidoreductaseClassifier'\n",
    "#    'ReadingFrameClassifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can now create any model config you want, plug it into a fastai learner along with your databunch, and train\n",
    "#lm\n",
    "#classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I don't want to deal with all the databunch/training stuff. What if I really just want to make some predictions on some data with a pretrained model? \n",
    "\n",
    "You can do that! Pretrained models for LookingGlass and associated transfer learning tasks can be downloaded in [release v1 of LookingGlass](https://github.com/ahoarfrost/LookingGlass/releases/tag/v1.0). The ones that end in 'export.pkl' were saved using the fastai 'export' function and can be loaded (with empty databunches) and used for inference directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the export pretrained models with load_learner (like in InterpretLG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
